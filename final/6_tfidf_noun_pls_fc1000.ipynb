{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf encode each train description (noun/all)\n",
    "# fit plsr with tfidf and imgf (noun/all -> fc1000/pool5)\n",
    "# query time, pred with plsr\n",
    "# fit knn with test gt imgf\n",
    "# find 20 nearest of query tfidf\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from autocorrect import spell\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFIDF nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(train_path, test_path, out_name, isNoun):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    corpus = []\n",
    "    for i in range(10000):\n",
    "        desc_file = open(train_path + str(i) + '.txt', 'r')\n",
    "        desc = ' '.join(desc_file.readlines())\n",
    "\n",
    "        tokens = tokenizer.tokenize(desc)\n",
    "        wordtags = pos_tag(tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            nouns = [word.lower() for word, pos in wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            nouns = [spell(token.lower()) for token in tokens]\n",
    "\n",
    "        nouns = [lmtzr.lemmatize(noun, \"v\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"n\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"a\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"r\") for noun in nouns]\n",
    "        nouns = [noun for noun in nouns if noun not in stopwords.words('english')]\n",
    "        corpus.append(' '.join(nouns)) \n",
    "\n",
    "    print(corpus[:5])   \n",
    "    print('Done processing training descriptions.')\n",
    "    \n",
    "    query_corpus = []\n",
    "    for i in range(2000): \n",
    "        query_file = open(test_path + str(i) + '.txt', 'r')\n",
    "        query_desc = ' '.join(query_file.readlines())\n",
    "\n",
    "        query_tokens = tokenizer.tokenize(query_desc)\n",
    "        query_wordtags = pos_tag(query_tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            query_nouns = [word.lower() for word, pos in query_wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            query_nouns = [spell(token.lower()) for token in query_tokens]\n",
    "\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"v\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"n\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"a\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"r\") for noun in query_nouns]\n",
    "        query_nouns = [noun for noun in query_nouns if noun not in stopwords.words('english')]\n",
    "        query_corpus.append(' '.join(query_nouns))\n",
    "    \n",
    "    print(query_corpus[:5])\n",
    "    print('Done processing query descriptions.')\n",
    "    \n",
    "    corpus_all = corpus + query_corpus\n",
    "    print('Merged.')\n",
    "    \n",
    "    np.save(out_name, corpus_all)\n",
    "    print('Saved corpus to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tfidf(corpus, out_name):\n",
    "    cv = CountVectorizer(min_df = 3)\n",
    "    X_all_bow = cv.fit_transform(corpus).toarray()\n",
    "    vocab = np.array(cv.get_feature_names())\n",
    "    transformer = TfidfTransformer()\n",
    "    X_all_tfidf = transformer.fit_transform(X_all_bow).toarray()\n",
    "\n",
    "    print(vocab)\n",
    "    print('vocab.shape:', vocab.shape)\n",
    "    print(X_all_tfidf[:10])\n",
    "    \n",
    "    np.save(out_name, np.asarray(X_all_tfidf))\n",
    "    print('Saved TFIDF to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TFIDF from preprocessing/X_nouns_tfidf.npy.\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(10000, 1994) (2000, 1994)\n"
     ]
    }
   ],
   "source": [
    "tfidf_path = 'preprocessing/X_nouns_tfidf.npy'\n",
    "if os.path.exists(tfidf_path):\n",
    "    tfidf_all = np.load(tfidf_path)\n",
    "    print('Loaded TFIDF from ' + tfidf_path + '.')\n",
    "    print(tfidf_all[:5])\n",
    "else:\n",
    "    corpus_all = process_corpus('data/descriptions_train/', 'data/descriptions_test/', 'corpus_nouns', False)\n",
    "    tfidf_all = process_tfidf(corpus_all, 'X_nouns_tfidf')\n",
    "\n",
    "tfidf_train = tfidf_all[:10000]\n",
    "tfidf_test = tfidf_all[10000:]\n",
    "print(tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fc1000, PCA on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_imgf(in_name, out_name):\n",
    "    imgf = {}\n",
    "    csvfile = open(in_name, 'r')\n",
    "    lines = csvfile.readlines()\n",
    "    for line in lines:\n",
    "        iid = int(line.split(\",\")[0].split(\"/\")[1].split(\".\")[0])\n",
    "        imgf[iid] = np.asarray([float(s) for s in line.split(\",\")[1:]])    \n",
    "\n",
    "    sorted_imgf = np.asarray([imgf[key] for key in sorted(imgf.keys())])\n",
    "    np.save(out_name, sorted_imgf)\n",
    "    print('Sorted ' + in_name + ' saved to ' + out_name + '.')\n",
    "    \n",
    "    return sorted_imgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image feature from preprocessing/sorted_fc1000.npy.\n",
      "Loaded image feature from preprocessing/sorted_fc1000_test.npy.\n"
     ]
    }
   ],
   "source": [
    "train_imf_path = 'preprocessing/sorted_fc1000.npy'\n",
    "if os.path.exists(train_imf_path):\n",
    "    sorted_fc1000 = np.load(train_imf_path)\n",
    "    print('Loaded image feature from ' + train_imf_path + '.')\n",
    "else:\n",
    "    sorted_fc1000 = process_imgf('data/features_train/features_resnet1000intermediate_train.csv', 'sorted_pool5')\n",
    "\n",
    "test_imgf_path = 'preprocessing/sorted_fc1000_test.npy'\n",
    "if os.path.exists(test_imgf_path):\n",
    "    sorted_fc1000_test = np.load(test_imgf_path)\n",
    "    print('Loaded image feature from ' + test_imgf_path + '.')\n",
    "else:\n",
    "    sorted_fc1000_test = process_imgf('data/features_test/features_resnet1000intermediate_test.csv', 'sorted_pool5_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "pca_noun_fc1000 = PCA(n_components=1000, svd_solver='auto')\n",
    "pca_noun_fc1000.fit(tfidf_train)\n",
    "tfidf_noun_train_PCA = pca_noun_fc1000.transform(tfidf_train)\n",
    "tfidf_noun_test_PCA = pca_noun_fc1000.transform(tfidf_test)\n",
    "\n",
    "print(tfidf_noun_train_PCA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PLSR 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_plsr(train, preds, n_components, max_iter, out_name):\n",
    "    plsr = PLSRegression(n_components=n_components, max_iter=max_iter)\n",
    "    plsr.fit(train, pred)\n",
    "    print('Done fitting PLSR.')\n",
    "    pickle.dump(plsr, open(out_name, 'wb'))\n",
    "    print('Saved PLSR ' + str(n_components) + ' to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PLSR from models/pls_noun_fc1000_400c.sav.\n"
     ]
    }
   ],
   "source": [
    "plsr_path = 'models/pls_noun_fc1000_400c.sav'\n",
    "if os.path.exists(plsr_path):\n",
    "    with open(plsr_path, 'rb') as f:\n",
    "        pls_noun_fc1000 = pickle.load(f)\n",
    "        print('Loaded PLSR from ' + plsr_path + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1161  922 1629 1273 1011 1969  619 1510 1480  112 1028 1276  700 1284\n",
      "  1535   95 1045 1069 1868 1181]\n",
      " [ 331 1559 1733  179 1742  428  144 1824   80 1871 1029 1601  571  529\n",
      "  1806  850 1199 1714  885 1006]\n",
      " [1183  634  838  579  159  953 1471  511  445 1724 1144  928  322  372\n",
      "  1084 1574  696 1716 1292  600]\n",
      " [ 825  770 1052  249 1215 1097 1594 1700  829 1429 1318 1423  358 1207\n",
      "   222  236 1396 1620 1765 1145]\n",
      " [1011  985  597 1262  849 1161 1069  619   46 1969 1510 1298 1384 1273\n",
      "   922  604  231  360 1452  132]\n",
      " [1335   51 1630  630  846 1700 1118 1088 1693  728  249 1429 1533  600\n",
      "  1292 1943  771 1889 1355 1913]\n",
      " [1660  429 1265 1874 1216 1855 1275 1790 1472  458 1693  771 1630  814\n",
      "   262 1943  623 1701 1584 1815]\n",
      " [ 975 1900 1857 1961  685 1249 1949 1270  689 1488 1085  448 1694 1285\n",
      "   987  409 1363  168  481  932]\n",
      " [1283  781 1927  537 1048 1004 1031  923  826 1197  419  403  727 1834\n",
      "   208 1348 1064    3  245 1896]\n",
      " [1700  940 1426 1429  845 1292  351 1479 1818 1355 1630  760 1927 1484\n",
      "  1913 1503  639  321 1296  834]]\n",
      "(2000, 20)\n"
     ]
    }
   ],
   "source": [
    "near = KNN(n_neighbors = 20, metric = 'cosine').fit(sorted_fc1000_test)\n",
    "preds = near.kneighbors(pls_noun_fc1000.predict(tfidf_noun_test_PCA), return_distance = False)\n",
    "print(preds[:10])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_submission(preds, out_name):\n",
    "    out = []\n",
    "    for pred in preds:\n",
    "        pred = [str(iid) + '.jpg' for iid in pred]\n",
    "        out.append(' '.join(pred))\n",
    "    print(out[:10])\n",
    "\n",
    "    out_files = []\n",
    "    for i in range(2000):\n",
    "        out_files.append(str(i)+'.txt')\n",
    "    with open(out_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Descritpion_ID', 'Top_20_Image_IDs'])\n",
    "        writer.writerows(zip(out_files, out))\n",
    "    print('Submission:', out_name)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1161.jpg 922.jpg 1629.jpg 1273.jpg 1011.jpg 1969.jpg 619.jpg 1510.jpg 1480.jpg 112.jpg 1028.jpg 1276.jpg 700.jpg 1284.jpg 1535.jpg 95.jpg 1045.jpg 1069.jpg 1868.jpg 1181.jpg', '331.jpg 1559.jpg 1733.jpg 179.jpg 1742.jpg 428.jpg 144.jpg 1824.jpg 80.jpg 1871.jpg 1029.jpg 1601.jpg 571.jpg 529.jpg 1806.jpg 850.jpg 1199.jpg 1714.jpg 885.jpg 1006.jpg', '1183.jpg 634.jpg 838.jpg 579.jpg 159.jpg 953.jpg 1471.jpg 511.jpg 445.jpg 1724.jpg 1144.jpg 928.jpg 322.jpg 372.jpg 1084.jpg 1574.jpg 696.jpg 1716.jpg 1292.jpg 600.jpg', '825.jpg 770.jpg 1052.jpg 249.jpg 1215.jpg 1097.jpg 1594.jpg 1700.jpg 829.jpg 1429.jpg 1318.jpg 1423.jpg 358.jpg 1207.jpg 222.jpg 236.jpg 1396.jpg 1620.jpg 1765.jpg 1145.jpg', '1011.jpg 985.jpg 597.jpg 1262.jpg 849.jpg 1161.jpg 1069.jpg 619.jpg 46.jpg 1969.jpg 1510.jpg 1298.jpg 1384.jpg 1273.jpg 922.jpg 604.jpg 231.jpg 360.jpg 1452.jpg 132.jpg', '1335.jpg 51.jpg 1630.jpg 630.jpg 846.jpg 1700.jpg 1118.jpg 1088.jpg 1693.jpg 728.jpg 249.jpg 1429.jpg 1533.jpg 600.jpg 1292.jpg 1943.jpg 771.jpg 1889.jpg 1355.jpg 1913.jpg', '1660.jpg 429.jpg 1265.jpg 1874.jpg 1216.jpg 1855.jpg 1275.jpg 1790.jpg 1472.jpg 458.jpg 1693.jpg 771.jpg 1630.jpg 814.jpg 262.jpg 1943.jpg 623.jpg 1701.jpg 1584.jpg 1815.jpg', '975.jpg 1900.jpg 1857.jpg 1961.jpg 685.jpg 1249.jpg 1949.jpg 1270.jpg 689.jpg 1488.jpg 1085.jpg 448.jpg 1694.jpg 1285.jpg 987.jpg 409.jpg 1363.jpg 168.jpg 481.jpg 932.jpg', '1283.jpg 781.jpg 1927.jpg 537.jpg 1048.jpg 1004.jpg 1031.jpg 923.jpg 826.jpg 1197.jpg 419.jpg 403.jpg 727.jpg 1834.jpg 208.jpg 1348.jpg 1064.jpg 3.jpg 245.jpg 1896.jpg', '1700.jpg 940.jpg 1426.jpg 1429.jpg 845.jpg 1292.jpg 351.jpg 1479.jpg 1818.jpg 1355.jpg 1630.jpg 760.jpg 1927.jpg 1484.jpg 1913.jpg 1503.jpg 639.jpg 321.jpg 1296.jpg 834.jpg']\n",
      "Submission: pls_noun_fc1000_400c.csv\n"
     ]
    }
   ],
   "source": [
    "format_submission(preds, 'pls_noun_fc1000_400c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance 2000 * 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 55.78720095  79.36845265  49.34022448 ...  85.9364658   75.19343979\n",
      "   93.23058402]\n",
      " [ 74.2828085   85.92724626  71.88138139 ... 100.69292505  88.54927705\n",
      "   50.86052267]\n",
      " [ 72.48326068  79.35288668  81.50713678 ...  64.38733666  50.45962306\n",
      "  115.50852714]\n",
      " [ 66.90411366  60.35452377  66.04305061 ...  63.36043533  41.21986485\n",
      "   94.82950316]\n",
      " [ 53.39099784  62.9862572   48.22853323 ...  69.91044423  55.18732816\n",
      "   83.12352766]]\n",
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "dist_fc = ed(pls_noun_fc1000.predict(tfidf_noun_test_PCA), sorted_fc1000_test)\n",
    "print(dist_fc[:5])\n",
    "print(dist_fc.shape)\n",
    "np.save('dist_fc', dist_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
