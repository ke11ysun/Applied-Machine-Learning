{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multilabel classifier, input = description, output = tags\n",
    "# for each image, knn find top 20 most visualy similar with resnet feature\n",
    "# query time, predict tags based on description\n",
    "# find most similar tag\n",
    "# find according image\n",
    "# find top 20 images related\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from autocorrect import spell\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tags(in_path, out_name, isTrain):\n",
    "    tags_pool = []\n",
    "    tags = []\n",
    "    if isTrain:\n",
    "        cnt = 10000\n",
    "    else:\n",
    "        cnt = 2000\n",
    "    for i in range(cnt):\n",
    "        tag_path = in_path + str(i) + '.txt'\n",
    "        tag_file = open(tag_path, 'r')\n",
    "\n",
    "        img_tag = []      \n",
    "        lines = tag_file.readlines()\n",
    "        for line in lines:\n",
    "            tag = line.strip(\"\\n\").split(\":\")[1].replace(' ', '')\n",
    "            img_tag.append(tag)\n",
    "            if tag not in tags_pool:\n",
    "                tags_pool.append(tag)\n",
    "        tags.append(img_tag)\n",
    "    print(tags_pool)\n",
    "    print(len(tags_pool))\n",
    "    pprint(tags)\n",
    "    print(len(tags))\n",
    "    \n",
    "    cv = CountVectorizer(vocabulary = tags_pool)\n",
    "    final_tags = [' '.join(tag) for tag in tags]\n",
    "    print(final_tags[:5])\n",
    "    tags_0_1 = cv.fit_transform(final_tags).toarray()\n",
    "    print(tags_0_1[:5])\n",
    "    \n",
    "    np.save(out_name, tags_0_1)\n",
    "    print('Saved tags_0_1 to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tags_0_1 from preprocessing/tags_train_0_1.npy.\n",
      "Loaded tags_0_1 from preprocessing/tags_2000_0_1.npy.\n"
     ]
    }
   ],
   "source": [
    "train_tags_path = 'preprocessing/tags_train_0_1.npy'\n",
    "if os.path.exists(train_tags_path):\n",
    "    tags_train_0_1 = np.load(train_tags_path)\n",
    "    print('Loaded tags_0_1 from ' + train_tags_path + '.')\n",
    "else:\n",
    "    tags_train_0_1 = process_tags('data/tags_train/', 'tags_train_0_1', True)\n",
    "    \n",
    "test_tags_path = 'preprocessing/tags_2000_0_1.npy'\n",
    "if os.path.exists(test_tags_path):\n",
    "    tags_2000_0_1 = np.load(test_tags_path)\n",
    "    print('Loaded tags_0_1 from ' + test_tags_path + '.')\n",
    "else:\n",
    "    tags_2000_0_1 = process_tags('data/tags_test/', 'tags_2000_0_1', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFIDF all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(train_path, test_path, out_name, isNoun):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    corpus = []\n",
    "    for i in range(10000):\n",
    "        desc_file = open(train_path + str(i) + '.txt', 'r')\n",
    "        desc = ' '.join(desc_file.readlines())\n",
    "\n",
    "        tokens = tokenizer.tokenize(desc)\n",
    "        wordtags = pos_tag(tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            nouns = [word.lower() for word, pos in wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            nouns = [spell(token.lower()) for token in tokens]\n",
    "\n",
    "        nouns = [lmtzr.lemmatize(noun, \"v\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"n\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"a\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"r\") for noun in nouns]\n",
    "        nouns = [noun for noun in nouns if noun not in stopwords.words('english')]\n",
    "        corpus.append(' '.join(nouns)) \n",
    "\n",
    "    print(corpus[:5])   \n",
    "    print('Done processing training descriptions.')\n",
    "    \n",
    "    query_corpus = []\n",
    "    for i in range(2000): \n",
    "        query_file = open(test_path + str(i) + '.txt', 'r')\n",
    "        query_desc = ' '.join(query_file.readlines())\n",
    "\n",
    "        query_tokens = tokenizer.tokenize(query_desc)\n",
    "        query_wordtags = pos_tag(query_tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            query_nouns = [word.lower() for word, pos in query_wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            query_nouns = [spell(token.lower()) for token in query_tokens]\n",
    "\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"v\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"n\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"a\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"r\") for noun in query_nouns]\n",
    "        query_nouns = [noun for noun in query_nouns if noun not in stopwords.words('english')]\n",
    "        query_corpus.append(' '.join(query_nouns))\n",
    "    \n",
    "    print(query_corpus[:5])\n",
    "    print('Done processing query descriptions.')\n",
    "    \n",
    "    corpus_all = corpus + query_corpus\n",
    "    print('Merged.')\n",
    "    \n",
    "    np.save(out_name, corpus_all)\n",
    "    print('Saved corpus to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tfidf(corpus, out_name):\n",
    "    cv = CountVectorizer(min_df = 3)\n",
    "    X_all_bow = cv.fit_transform(corpus).toarray()\n",
    "    vocab = np.array(cv.get_feature_names())\n",
    "    transformer = TfidfTransformer()\n",
    "    X_all_tfidf = transformer.fit_transform(X_all_bow).toarray()\n",
    "\n",
    "    print(vocab)\n",
    "    print('vocab.shape:', vocab.shape)\n",
    "    print(X_all_tfidf[:10])\n",
    "    \n",
    "    np.save(out_name, np.asarray(X_all_tfidf))\n",
    "    print('Saved TFIDF to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TFIDF from file.\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(10000, 3311) (2000, 3311)\n"
     ]
    }
   ],
   "source": [
    "tfidf_path = 'preprocessing/X_all_tfidf.npy'\n",
    "if os.path.exists(tfidf_path):\n",
    "    tfidf_all = np.load(tfidf_path)\n",
    "    print('Loaded TFIDF from file.')\n",
    "    print(tfidf_all[:5])\n",
    "else:\n",
    "    corpus_all = process_corpus('data/descriptions_train/', 'data/descriptions_test/', 'corpus_all', False)\n",
    "    tfidf_all = process_tfidf(corpus_all, 'X_all_tfidf')\n",
    "\n",
    "tfidf_train = tfidf_all[:10000]\n",
    "tfidf_test = tfidf_all[10000:]\n",
    "print(tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt\n",
    "from sklearn.decomposition import PCA\n",
    "pca_tfidf = PCA(n_components=1000, svd_solver='auto')\n",
    "pca_tfidf.fit(tfidf_all)\n",
    "X_all_tfidf_pca = pca_tfidf.transform(tfidf_all)\n",
    "X_tfidf_pca = X_all_tfidf_pca[:10000]\n",
    "X_query_tfidf_pca = X_all_tfidf_pca[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression, in: description TFIDFs, out: pred tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 80)\n"
     ]
    }
   ],
   "source": [
    "predict_tags = []\n",
    "for i in range(80):\n",
    "    y = tags_train_0_1[:,i]\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(tfidf_train, y)\n",
    "    tag = clf.predict(tfidf_test)\n",
    "    predict_tags.append(tag)\n",
    "\n",
    "predict_tags_trans = np.asarray(predict_tags).T\n",
    "print(predict_tags_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 80)\n"
     ]
    }
   ],
   "source": [
    "# opt\n",
    "predict_tags_pca = []\n",
    "for i in range(80):\n",
    "    y = tags_train_0_1[:,i]\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_tfidf_pca, y)\n",
    "    tag = clf.predict(X_query_tfidf_pca)\n",
    "    predict_tags_pca.append(tag)\n",
    "\n",
    "predict_tags_pca_trans = np.asarray(predict_tags_pca).T\n",
    "print(predict_tags_pca_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN, fit with all test tags, in: query tag, out: img id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1862 1698  359 ...  388  281  465]\n",
      " [ 589  833 1341 ...  171  948  763]\n",
      " [1840  649 1724 ... 1089  283  953]\n",
      " ...\n",
      " [ 104  199 1481 ...  674   58  429]\n",
      " [1429  141 1660 ...  269  151  145]\n",
      " [ 926 1342  729 ...  308  468 1135]]\n",
      "(2000, 20)\n"
     ]
    }
   ],
   "source": [
    "near = KNN(n_neighbors = 20).fit(tags_2000_0_1)\n",
    "preds = near.kneighbors(predict_tags_trans, return_distance = False)\n",
    "print(preds)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 20)\n"
     ]
    }
   ],
   "source": [
    "# opt\n",
    "near_pca = KNN(n_neighbors = 20).fit(tags_2000_0_1)\n",
    "preds_pca = near_pca.kneighbors(predict_tags_pca_trans, return_distance = False)\n",
    "print(preds_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (bad) 1NN, in: query tag, out: nearest img id; KNN, fit with imgf, in: nearest imgf, out: 20 img ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_imgf(in_name, out_name):\n",
    "    imgf = {}\n",
    "    csvfile = open(in_name, 'r')\n",
    "    lines = csvfile.readlines()\n",
    "    for line in lines:\n",
    "        iid = int(line.split(\",\")[0].split(\"/\")[1].split(\".\")[0])\n",
    "        imgf[iid] = np.asarray([float(s) for s in line.split(\",\")[1:]])    \n",
    "\n",
    "    sorted_imgf = np.asarray([imgf[key] for key in sorted(imgf.keys())])\n",
    "    np.save(out_name, sorted_imgf)\n",
    "    print('Sorted ' + in_name + ' saved to ' + out_name + '.')\n",
    "    \n",
    "    return sorted_imgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image feature from preprocessing/sorted_fc1000.npy.\n",
      "Loaded image feature from preprocessing/sorted_fc1000_test.npy.\n"
     ]
    }
   ],
   "source": [
    "train_imf_path = 'preprocessing/sorted_fc1000.npy'\n",
    "if os.path.exists(train_imf_path):\n",
    "    sorted_fc1000 = np.load(train_imf_path)\n",
    "    print('Loaded image feature from ' + train_imf_path + '.')\n",
    "else:\n",
    "    sorted_fc1000 = process_imgf('data/features_train/features_resnet1000intermediate_train.csv', 'sorted_pool5')\n",
    "\n",
    "test_imgf_path = 'preprocessing/sorted_fc1000_test.npy'\n",
    "if os.path.exists(test_imgf_path):\n",
    "    sorted_fc1000_test = np.load(test_imgf_path)\n",
    "    print('Loaded image feature from ' + test_imgf_path + '.')\n",
    "else:\n",
    "    sorted_fc1000_test = process_imgf('data/features_test/features_resnet1000intermediate_test.csv', 'sorted_pool5_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "nearest = KNN(n_neighbors = 1).fit(tags_2000_0_1)\n",
    "preds_1 = nearest.kneighbors(predict_tags_trans, return_distance = False)\n",
    "print(preds_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1698   68 1243  916 1816 1092  359 1451  912  721 1384  292  670 1231\n",
      "   742  164  637  416  808 1076]\n",
      " [ 589 1068 1147  818  835 1199 1143 1714 1054 1645  201  107  261 1184\n",
      "    80  482  457  668 1733 1102]\n",
      " [ 184 1027 1475 1648 1471  860  904 1421  696 1482  496  338 1962   42\n",
      "  1437 1023 1040 1183 1084 1413]\n",
      " [  75 1035 1112  799 1513 1315  427 1486 1850  153  677 1279 1394  686\n",
      "   661  770  451  417   66  930]\n",
      " [  36  669  683    3 1040  360   46  511 1919  153 1479  969  881 1413\n",
      "  1902  604   68  781  588 1180]\n",
      " [ 274 1913 1558 1886 1837 1882 1254  829   77  846   81 1974 1206 1850\n",
      "  1339  268 1633  947 1018  673]\n",
      " [  33 1265 1191 1020 1885  244 1153  367 1651  489  249  977  309 1655\n",
      "  1044  899 1940  458 1889  414]\n",
      " [1949  975 1363 1857  932 1961 1694  689 1085 1123 1900 1488 1249  770\n",
      "   409  448  685  987 1270  805]\n",
      " [ 266 1600 1031 1644 1348 1389  848  719 1004 1896 1261  923 1901 1336\n",
      "  1557  459 1656  343 1010 1847]\n",
      " [ 274 1913 1558 1886 1837 1882 1254  829   77  846   81 1974 1206 1850\n",
      "  1339  268 1633  947 1018  673]]\n"
     ]
    }
   ],
   "source": [
    "near_img = KNN(n_neighbors = 20).fit(sorted_fc1000_test)\n",
    "preds_img = []\n",
    "for i in range(2000):\n",
    "    preds_img.append(near_img.kneighbors(sorted_fc1000_test[preds_1[i]], return_distance = False))\n",
    "preds_img = np.asarray(preds_img).squeeze()\n",
    "print(preds_img[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_submission(preds, out_name):\n",
    "    out = []\n",
    "    for pred in preds:\n",
    "        pred = [str(iid) + '.jpg' for iid in pred]\n",
    "        out.append(' '.join(pred))\n",
    "    print(out[:10])\n",
    "\n",
    "    out_files = []\n",
    "    for i in range(2000):\n",
    "        out_files.append(str(i)+'.txt')\n",
    "    with open(out_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Descritpion_ID', 'Top_20_Image_IDs'])\n",
    "        writer.writerows(zip(out_files, out))\n",
    "    print('Submission:', out_name)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1698.jpg 68.jpg 1243.jpg 916.jpg 1816.jpg 1092.jpg 359.jpg 1451.jpg 912.jpg 721.jpg 1384.jpg 292.jpg 670.jpg 1231.jpg 742.jpg 164.jpg 637.jpg 416.jpg 808.jpg 1076.jpg', '589.jpg 1068.jpg 1147.jpg 818.jpg 835.jpg 1199.jpg 1143.jpg 1714.jpg 1054.jpg 1645.jpg 201.jpg 107.jpg 261.jpg 1184.jpg 80.jpg 482.jpg 457.jpg 668.jpg 1733.jpg 1102.jpg', '184.jpg 1027.jpg 1475.jpg 1648.jpg 1471.jpg 860.jpg 904.jpg 1421.jpg 696.jpg 1482.jpg 496.jpg 338.jpg 1962.jpg 42.jpg 1437.jpg 1023.jpg 1040.jpg 1183.jpg 1084.jpg 1413.jpg', '75.jpg 1035.jpg 1112.jpg 799.jpg 1513.jpg 1315.jpg 427.jpg 1486.jpg 1850.jpg 153.jpg 677.jpg 1279.jpg 1394.jpg 686.jpg 661.jpg 770.jpg 451.jpg 417.jpg 66.jpg 930.jpg', '36.jpg 669.jpg 683.jpg 3.jpg 1040.jpg 360.jpg 46.jpg 511.jpg 1919.jpg 153.jpg 1479.jpg 969.jpg 881.jpg 1413.jpg 1902.jpg 604.jpg 68.jpg 781.jpg 588.jpg 1180.jpg', '274.jpg 1913.jpg 1558.jpg 1886.jpg 1837.jpg 1882.jpg 1254.jpg 829.jpg 77.jpg 846.jpg 81.jpg 1974.jpg 1206.jpg 1850.jpg 1339.jpg 268.jpg 1633.jpg 947.jpg 1018.jpg 673.jpg', '33.jpg 1265.jpg 1191.jpg 1020.jpg 1885.jpg 244.jpg 1153.jpg 367.jpg 1651.jpg 489.jpg 249.jpg 977.jpg 309.jpg 1655.jpg 1044.jpg 899.jpg 1940.jpg 458.jpg 1889.jpg 414.jpg', '1949.jpg 975.jpg 1363.jpg 1857.jpg 932.jpg 1961.jpg 1694.jpg 689.jpg 1085.jpg 1123.jpg 1900.jpg 1488.jpg 1249.jpg 770.jpg 409.jpg 448.jpg 685.jpg 987.jpg 1270.jpg 805.jpg', '266.jpg 1600.jpg 1031.jpg 1644.jpg 1348.jpg 1389.jpg 848.jpg 719.jpg 1004.jpg 1896.jpg 1261.jpg 923.jpg 1901.jpg 1336.jpg 1557.jpg 459.jpg 1656.jpg 343.jpg 1010.jpg 1847.jpg', '274.jpg 1913.jpg 1558.jpg 1886.jpg 1837.jpg 1882.jpg 1254.jpg 829.jpg 77.jpg 846.jpg 81.jpg 1974.jpg 1206.jpg 1850.jpg 1339.jpg 268.jpg 1633.jpg 947.jpg 1018.jpg 673.jpg']\n",
      "Submission: 1tag20imgs.csv\n"
     ]
    }
   ],
   "source": [
    "format_submission(preds_img, '1tag20imgs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
