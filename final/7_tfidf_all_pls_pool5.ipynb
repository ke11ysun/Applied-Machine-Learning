{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from autocorrect import spell\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFIDF all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(train_path, test_path, out_name, isNoun):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    corpus = []\n",
    "    for i in range(10000):\n",
    "        desc_file = open(train_path + str(i) + '.txt', 'r')\n",
    "        desc = ' '.join(desc_file.readlines())\n",
    "\n",
    "        tokens = tokenizer.tokenize(desc)\n",
    "        wordtags = pos_tag(tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            nouns = [word.lower() for word, pos in wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            nouns = [spell(token.lower()) for token in tokens]\n",
    "\n",
    "        nouns = [lmtzr.lemmatize(noun, \"v\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"n\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"a\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"r\") for noun in nouns]\n",
    "        nouns = [noun for noun in nouns if noun not in stopwords.words('english')]\n",
    "        corpus.append(' '.join(nouns)) \n",
    "\n",
    "    print(corpus[:5])   \n",
    "    print('Done processing training descriptions.')\n",
    "    \n",
    "    query_corpus = []\n",
    "    for i in range(2000): \n",
    "        query_file = open(test_path + str(i) + '.txt', 'r')\n",
    "        query_desc = ' '.join(query_file.readlines())\n",
    "\n",
    "        query_tokens = tokenizer.tokenize(query_desc)\n",
    "        query_wordtags = pos_tag(query_tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            query_nouns = [word.lower() for word, pos in query_wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            query_nouns = [spell(token.lower()) for token in query_tokens]\n",
    "\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"v\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"n\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"a\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"r\") for noun in query_nouns]\n",
    "        query_nouns = [noun for noun in query_nouns if noun not in stopwords.words('english')]\n",
    "        query_corpus.append(' '.join(query_nouns))\n",
    "    \n",
    "    print(query_corpus[:5])\n",
    "    print('Done processing query descriptions.')\n",
    "    \n",
    "    corpus_all = corpus + query_corpus\n",
    "    print('Merged.')\n",
    "    \n",
    "    np.save(out_name, corpus_all)\n",
    "    print('Saved corpus to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tfidf(corpus, out_name):\n",
    "    cv = CountVectorizer(min_df = 3)\n",
    "    X_all_bow = cv.fit_transform(corpus).toarray()\n",
    "    vocab = np.array(cv.get_feature_names())\n",
    "    transformer = TfidfTransformer()\n",
    "    X_all_tfidf = transformer.fit_transform(X_all_bow).toarray()\n",
    "\n",
    "    print(vocab)\n",
    "    print('vocab.shape:', vocab.shape)\n",
    "    print(X_all_tfidf[:10])\n",
    "    \n",
    "    np.save(out_name, np.asarray(X_all_tfidf))\n",
    "    print('Saved TFIDF to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TFIDF from file.\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(10000, 3311) (2000, 3311)\n"
     ]
    }
   ],
   "source": [
    "tfidf_path = 'preprocessing/X_all_tfidf.npy'\n",
    "if os.path.exists(tfidf_path):\n",
    "    tfidf_all = np.load(tfidf_path)\n",
    "    print('Loaded TFIDF from file.')\n",
    "    print(tfidf_all[:5])\n",
    "else:\n",
    "    corpus_all = process_corpus('data/descriptions_train/', 'data/descriptions_test/', 'corpus_all', False)\n",
    "    tfidf_all = process_tfidf(corpus_all, 'X_all_tfidf')\n",
    "\n",
    "tfidf_train = tfidf_all[:10000]\n",
    "tfidf_test = tfidf_all[10000:]\n",
    "print(tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pool5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_imgf(in_name, out_name):\n",
    "    imgf = {}\n",
    "    csvfile = open(in_name, 'r')\n",
    "    lines = csvfile.readlines()\n",
    "    for line in lines:\n",
    "        iid = int(line.split(\",\")[0].split(\"/\")[1].split(\".\")[0])\n",
    "        imgf[iid] = np.asarray([float(s) for s in line.split(\",\")[1:]])    \n",
    "\n",
    "    sorted_imgf = np.asarray([imgf[key] for key in sorted(imgf.keys())])\n",
    "    np.save(out_name, sorted_imgf)\n",
    "    print('Sorted ' + in_name + ' saved to ' + out_name + '.')\n",
    "    \n",
    "    return sorted_imgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sorted_pool5 from file.\n",
      "Loaded sorted_pool5_test from file.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('preprocessing/sorted_pool5.npy'):\n",
    "    sorted_pool5 = np.load('preprocessing/sorted_pool5.npy')\n",
    "    print('Loaded sorted_pool5 from file.')\n",
    "else:\n",
    "    sorted_pool5 = process_imgf('data/features_train/features_resnet1000intermediate_train.csv', 'sorted_pool5')\n",
    "\n",
    "if os.path.exists('preprocessing/sorted_pool5_test.npy'):\n",
    "    sorted_pool5_test = np.load('preprocessing/sorted_pool5_test.npy')\n",
    "    print('Loaded sorted_pool5_test from file.')\n",
    "else:\n",
    "    sorted_pool5_test = process_imgf('data/features_test/features_resnet1000intermediate_test.csv', 'sorted_pool5_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PLSR 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_plsr(train, preds, n_components, max_iter, out_name):\n",
    "    plsr = PLSRegression(n_components=n_components, max_iter=max_iter)\n",
    "    plsr.fit(train, pred)\n",
    "    print('Done fitting PLSR.')\n",
    "    pickle.dump(plsr, open(out_name, 'wb'))\n",
    "    print('Saved PLSR ' + str(n_components) + ' to ' + out_name + '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PLSR from models/pls_all_pool5_2048c.sav.\n"
     ]
    }
   ],
   "source": [
    "plsr_path = 'models/pls_all_pool5_2048c.sav'\n",
    "if os.path.exists(plsr_path):\n",
    "    with open(plsr_path, 'rb') as f:\n",
    "        pls_all_pool5 = pickle.load(f)\n",
    "        print('Loaded PLSR from ' + plsr_path + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1161 1380  329  714    3  760  290  153  105 1011  985  540  931  231\n",
      "    46 1479 1982  683 1107 1367]\n",
      " [1989 1714 1907  355  674  331 1871  820 1615 1134 1559 1742  997  107\n",
      "  1199   57  428  482 1068  833]\n",
      " [ 634  838 1183 1292 1724 1471  159   42  904  511  781  445  598  600\n",
      "  1896  953  696 1866  979  249]\n",
      " [ 770 1514 1620  829  469  799 1513   75 1315 1145 1207 1486 1609  451\n",
      "  1396 1035 1727   26 1404 1414]\n",
      " [ 808 1384  112  608 1161  849  150 1629 1606  216 1273 1597  956 1603\n",
      "  1446 1015  206 1913   21   46]\n",
      " [1335  771 1693 1145 1292 1837 1943  814   51  249 1913 1630  846 1088\n",
      "    77  330  492  829 1429 1700]\n",
      " [1855 1701 1660 1275 1216  414 1472  583  429 1151  655 1311  262 1289\n",
      "   718  217 1815 1377  996 1458]\n",
      " [ 975 1857  689 1961  685 1488 1249 1949 1694 1085 1302 1270  448  481\n",
      "   987  168 1328 1239 1285  528]\n",
      " [1446  537  781  923  775  597 1700 1004 1927 1283  361 1779 1228  659\n",
      "   719  132    3 1261 1292  103]\n",
      " [1913 1837 1145 1446 1700 1429 1943  845   77 1292 1335 1818 1254  814\n",
      "   829 1630  249  940 1479  846]]\n",
      "(2000, 20)\n"
     ]
    }
   ],
   "source": [
    "near = KNN(n_neighbors = 20, metric = 'cosine').fit(sorted_pool5_test)\n",
    "preds = near.kneighbors(pls_all_pool5.predict(tfidf_test), return_distance = False)\n",
    "print(preds[:10])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_submission(preds, out_name):\n",
    "    out = []\n",
    "    for pred in preds:\n",
    "        pred = [str(iid) + '.jpg' for iid in pred]\n",
    "        out.append(' '.join(pred))\n",
    "    print(out[:10])\n",
    "\n",
    "    out_files = []\n",
    "    for i in range(2000):\n",
    "        out_files.append(str(i)+'.txt')\n",
    "    with open(out_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Descritpion_ID', 'Top_20_Image_IDs'])\n",
    "        writer.writerows(zip(out_files, out))\n",
    "    print('Submission:', out_name)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1161.jpg 1380.jpg 329.jpg 714.jpg 3.jpg 760.jpg 290.jpg 153.jpg 105.jpg 1011.jpg 985.jpg 540.jpg 931.jpg 231.jpg 46.jpg 1479.jpg 1982.jpg 683.jpg 1107.jpg 1367.jpg', '1989.jpg 1714.jpg 1907.jpg 355.jpg 674.jpg 331.jpg 1871.jpg 820.jpg 1615.jpg 1134.jpg 1559.jpg 1742.jpg 997.jpg 107.jpg 1199.jpg 57.jpg 428.jpg 482.jpg 1068.jpg 833.jpg', '634.jpg 838.jpg 1183.jpg 1292.jpg 1724.jpg 1471.jpg 159.jpg 42.jpg 904.jpg 511.jpg 781.jpg 445.jpg 598.jpg 600.jpg 1896.jpg 953.jpg 696.jpg 1866.jpg 979.jpg 249.jpg', '770.jpg 1514.jpg 1620.jpg 829.jpg 469.jpg 799.jpg 1513.jpg 75.jpg 1315.jpg 1145.jpg 1207.jpg 1486.jpg 1609.jpg 451.jpg 1396.jpg 1035.jpg 1727.jpg 26.jpg 1404.jpg 1414.jpg', '808.jpg 1384.jpg 112.jpg 608.jpg 1161.jpg 849.jpg 150.jpg 1629.jpg 1606.jpg 216.jpg 1273.jpg 1597.jpg 956.jpg 1603.jpg 1446.jpg 1015.jpg 206.jpg 1913.jpg 21.jpg 46.jpg', '1335.jpg 771.jpg 1693.jpg 1145.jpg 1292.jpg 1837.jpg 1943.jpg 814.jpg 51.jpg 249.jpg 1913.jpg 1630.jpg 846.jpg 1088.jpg 77.jpg 330.jpg 492.jpg 829.jpg 1429.jpg 1700.jpg', '1855.jpg 1701.jpg 1660.jpg 1275.jpg 1216.jpg 414.jpg 1472.jpg 583.jpg 429.jpg 1151.jpg 655.jpg 1311.jpg 262.jpg 1289.jpg 718.jpg 217.jpg 1815.jpg 1377.jpg 996.jpg 1458.jpg', '975.jpg 1857.jpg 689.jpg 1961.jpg 685.jpg 1488.jpg 1249.jpg 1949.jpg 1694.jpg 1085.jpg 1302.jpg 1270.jpg 448.jpg 481.jpg 987.jpg 168.jpg 1328.jpg 1239.jpg 1285.jpg 528.jpg', '1446.jpg 537.jpg 781.jpg 923.jpg 775.jpg 597.jpg 1700.jpg 1004.jpg 1927.jpg 1283.jpg 361.jpg 1779.jpg 1228.jpg 659.jpg 719.jpg 132.jpg 3.jpg 1261.jpg 1292.jpg 103.jpg', '1913.jpg 1837.jpg 1145.jpg 1446.jpg 1700.jpg 1429.jpg 1943.jpg 845.jpg 77.jpg 1292.jpg 1335.jpg 1818.jpg 1254.jpg 814.jpg 829.jpg 1630.jpg 249.jpg 940.jpg 1479.jpg 846.jpg']\n",
      "Submission: pls_all_pool5_2048c.csv\n"
     ]
    }
   ],
   "source": [
    "format_submission(preds, 'pls_all_pool5_2048c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance 2000 * 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.64242728 22.74472159 18.80300624 ... 21.92569311 21.69261115\n",
      "  25.36181337]\n",
      " [25.86516698 27.40234023 27.99033553 ... 29.47187955 28.39413123\n",
      "  23.46640576]\n",
      " [22.62722082 22.1714028  23.53803141 ... 21.78323414 18.3948048\n",
      "  28.21897581]\n",
      " [22.53819982 18.96064358 22.29525381 ... 21.57259255 16.73993491\n",
      "  26.92714481]\n",
      " [23.16323099 24.0392422  21.80058644 ... 24.55130498 23.17148436\n",
      "  26.97140383]]\n",
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "dist_pool = ed(pls_all_pool5.predict(tfidf_test), sorted_pool5_test)\n",
    "print(dist_pool[:5])\n",
    "print(dist_pool.shape)\n",
    "np.save('dist_pool', dist_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
