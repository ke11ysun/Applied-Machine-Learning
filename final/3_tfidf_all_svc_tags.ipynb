{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multilabel classifier, input = description, output = tags\n",
    "# for each image, knn find top 20 most visualy similar with resnet feature\n",
    "# query time, predict tags based on description\n",
    "# find most similar tag\n",
    "# find according image\n",
    "# find top 20 images related\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from autocorrect import spell\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tags(in_path, out_name, isTrain):\n",
    "    tags_pool = []\n",
    "    tags = []\n",
    "    if isTrain:\n",
    "        cnt = 10000\n",
    "    else:\n",
    "        cnt = 2000\n",
    "    for i in range(cnt):\n",
    "        tag_path = in_path + str(i) + '.txt'\n",
    "        tag_file = open(tag_path, 'r')\n",
    "\n",
    "        img_tag = []      \n",
    "        lines = tag_file.readlines()\n",
    "        for line in lines:\n",
    "            tag = line.strip(\"\\n\").split(\":\")[1].replace(' ', '')\n",
    "            img_tag.append(tag)\n",
    "            if tag not in tags_pool:\n",
    "                tags_pool.append(tag)\n",
    "        tags.append(img_tag)\n",
    "    print(tags_pool)\n",
    "    print(len(tags_pool))\n",
    "    pprint(tags)\n",
    "    print(len(tags))\n",
    "    \n",
    "    cv = CountVectorizer(vocabulary = tags_pool)\n",
    "    final_tags = [' '.join(tag) for tag in tags]\n",
    "    print(final_tags[:5])\n",
    "    tags_0_1 = cv.fit_transform(final_tags).toarray()\n",
    "    print(tags_0_1[:5])\n",
    "    \n",
    "    np.save(out_name, tags_0_1)\n",
    "    print('Saved tags_0_1 to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tags_0_1 from preprocessing/tags_train_0_1.npy.\n",
      "Loaded tags_0_1 from preprocessing/tags_2000_0_1.npy.\n"
     ]
    }
   ],
   "source": [
    "train_tags_path = 'preprocessing/tags_train_0_1.npy'\n",
    "if os.path.exists(train_tags_path):\n",
    "    tags_train_0_1 = np.load(train_tags_path)\n",
    "    print('Loaded tags_0_1 from ' + train_tags_path + '.')\n",
    "else:\n",
    "    tags_train_0_1 = process_tags('data/tags_train/', 'tags_train_0_1', True)\n",
    "    \n",
    "test_tags_path = 'preprocessing/tags_2000_0_1.npy'\n",
    "if os.path.exists(test_tags_path):\n",
    "    tags_2000_0_1 = np.load(test_tags_path)\n",
    "    print('Loaded tags_0_1 from ' + test_tags_path + '.')\n",
    "else:\n",
    "    tags_2000_0_1 = process_tags('data/tags_test/', 'tags_2000_0_1', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFIDF all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(train_path, test_path, out_name, isNoun):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    corpus = []\n",
    "    for i in range(10000):\n",
    "        desc_file = open(train_path + str(i) + '.txt', 'r')\n",
    "        desc = ' '.join(desc_file.readlines())\n",
    "\n",
    "        tokens = tokenizer.tokenize(desc)\n",
    "        wordtags = pos_tag(tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            nouns = [word.lower() for word, pos in wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            nouns = [spell(token.lower()) for token in tokens]\n",
    "\n",
    "        nouns = [lmtzr.lemmatize(noun, \"v\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"n\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"a\") for noun in nouns]\n",
    "        nouns = [lmtzr.lemmatize(noun, \"r\") for noun in nouns]\n",
    "        nouns = [noun for noun in nouns if noun not in stopwords.words('english')]\n",
    "        corpus.append(' '.join(nouns)) \n",
    "\n",
    "    print(corpus[:5])   \n",
    "    print('Done processing training descriptions.')\n",
    "    \n",
    "    query_corpus = []\n",
    "    for i in range(2000): \n",
    "        query_file = open(test_path + str(i) + '.txt', 'r')\n",
    "        query_desc = ' '.join(query_file.readlines())\n",
    "\n",
    "        query_tokens = tokenizer.tokenize(query_desc)\n",
    "        query_wordtags = pos_tag(query_tokens)\n",
    "        \n",
    "        if isNoun:\n",
    "            query_nouns = [word.lower() for word, pos in query_wordtags if (pos == 'NN')]\n",
    "        else:\n",
    "            query_nouns = [spell(token.lower()) for token in query_tokens]\n",
    "\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"v\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"n\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"a\") for noun in query_nouns]\n",
    "        query_nouns = [lmtzr.lemmatize(noun, \"r\") for noun in query_nouns]\n",
    "        query_nouns = [noun for noun in query_nouns if noun not in stopwords.words('english')]\n",
    "        query_corpus.append(' '.join(query_nouns))\n",
    "    \n",
    "    print(query_corpus[:5])\n",
    "    print('Done processing query descriptions.')\n",
    "    \n",
    "    corpus_all = corpus + query_corpus\n",
    "    print('Merged.')\n",
    "    \n",
    "    np.save(out_name, corpus_all)\n",
    "    print('Saved corpus to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tfidf(corpus, out_name):\n",
    "    cv = CountVectorizer(min_df = 3)\n",
    "    X_all_bow = cv.fit_transform(corpus).toarray()\n",
    "    vocab = np.array(cv.get_feature_names())\n",
    "    transformer = TfidfTransformer()\n",
    "    X_all_tfidf = transformer.fit_transform(X_all_bow).toarray()\n",
    "\n",
    "    print(vocab)\n",
    "    print('vocab.shape:', vocab.shape)\n",
    "    print(X_all_tfidf[:10])\n",
    "    \n",
    "    np.save(out_name, np.asarray(X_all_tfidf))\n",
    "    print('Saved TFIDF to ' + out_name + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TFIDF from file.\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(10000, 3311) (2000, 3311)\n"
     ]
    }
   ],
   "source": [
    "tfidf_path = 'preprocessing/X_all_tfidf.npy'\n",
    "if os.path.exists(tfidf_path):\n",
    "    tfidf_all = np.load(tfidf_path)\n",
    "    print('Loaded TFIDF from file.')\n",
    "    print(tfidf_all[:5])\n",
    "else:\n",
    "    corpus_all = process_corpus('data/descriptions_train/', 'data/descriptions_test/', 'corpus_all', False)\n",
    "    tfidf_all = process_tfidf(corpus_all, 'X_all_tfidf')\n",
    "\n",
    "tfidf_train = tfidf_all[:10000]\n",
    "tfidf_test = tfidf_all[10000:]\n",
    "print(tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM, in: description TFIDFs, out: pred tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 80)\n"
     ]
    }
   ],
   "source": [
    "predict_tags = []\n",
    "for i in range(80):\n",
    "    y = tags_train_0_1[:,i]\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(tfidf_train, y)\n",
    "    tag = clf.predict(tfidf_test)\n",
    "    predict_tags.append(tag)\n",
    "\n",
    "predict_tags_trans = np.asarray(predict_tags).T\n",
    "print(predict_tags_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN, fit with all test tags, in: query tag, out: img id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1862 1698  359 ...  388  281  465]\n",
      " [ 589  833 1341 ...  171  948  763]\n",
      " [ 720  598 1107 ... 1464  228 1493]\n",
      " ...\n",
      " [ 104  199 1481 ...  674   58  429]\n",
      " [1660 1135 1535 ...  210  226  243]\n",
      " [ 926 1342  729 ...  308  468 1135]]\n",
      "(2000, 20)\n"
     ]
    }
   ],
   "source": [
    "near = KNN(n_neighbors = 20).fit(tags_2000_0_1)\n",
    "preds = near.kneighbors(predict_tags_trans, return_distance = False)\n",
    "print(preds)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_submission(preds, out_name):\n",
    "    out = []\n",
    "    for pred in preds:\n",
    "        pred = [str(iid) + '.jpg' for iid in pred]\n",
    "        out.append(' '.join(pred))\n",
    "    print(out[:10])\n",
    "\n",
    "    out_files = []\n",
    "    for i in range(2000):\n",
    "        out_files.append(str(i)+'.txt')\n",
    "    with open(out_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Descritpion_ID', 'Top_20_Image_IDs'])\n",
    "        writer.writerows(zip(out_files, out))\n",
    "    print('Submission:', out_name)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1862.jpg 1698.jpg 359.jpg 1231.jpg 1131.jpg 1743.jpg 942.jpg 1479.jpg 36.jpg 124.jpg 292.jpg 425.jpg 381.jpg 141.jpg 250.jpg 484.jpg 364.jpg 388.jpg 281.jpg 465.jpg', '589.jpg 833.jpg 1341.jpg 1907.jpg 1156.jpg 1529.jpg 1071.jpg 562.jpg 121.jpg 607.jpg 753.jpg 758.jpg 956.jpg 809.jpg 869.jpg 860.jpg 452.jpg 171.jpg 948.jpg 763.jpg', '720.jpg 598.jpg 1107.jpg 953.jpg 1713.jpg 159.jpg 1955.jpg 545.jpg 1067.jpg 272.jpg 456.jpg 755.jpg 662.jpg 1909.jpg 43.jpg 1651.jpg 1060.jpg 1464.jpg 228.jpg 1493.jpg', '654.jpg 1865.jpg 75.jpg 1513.jpg 1922.jpg 800.jpg 836.jpg 1035.jpg 1486.jpg 397.jpg 799.jpg 90.jpg 686.jpg 930.jpg 897.jpg 1135.jpg 1220.jpg 469.jpg 152.jpg 66.jpg', '292.jpg 1231.jpg 1743.jpg 942.jpg 36.jpg 359.jpg 1131.jpg 311.jpg 95.jpg 152.jpg 381.jpg 388.jpg 141.jpg 760.jpg 250.jpg 1026.jpg 50.jpg 897.jpg 949.jpg 484.jpg', '51.jpg 1322.jpg 1118.jpg 1335.jpg 887.jpg 1045.jpg 771.jpg 1287.jpg 554.jpg 703.jpg 354.jpg 814.jpg 334.jpg 46.jpg 699.jpg 274.jpg 492.jpg 614.jpg 526.jpg 330.jpg', '33.jpg 1009.jpg 712.jpg 996.jpg 458.jpg 718.jpg 655.jpg 910.jpg 259.jpg 358.jpg 486.jpg 489.jpg 535.jpg 555.jpg 786.jpg 908.jpg 156.jpg 402.jpg 414.jpg 217.jpg', '1949.jpg 685.jpg 627.jpg 1218.jpg 481.jpg 897.jpg 987.jpg 1085.jpg 152.jpg 528.jpg 689.jpg 756.jpg 168.jpg 141.jpg 1155.jpg 1135.jpg 975.jpg 448.jpg 136.jpg 409.jpg', '1557.jpg 719.jpg 1948.jpg 1397.jpg 1622.jpg 134.jpg 419.jpg 266.jpg 1346.jpg 661.jpg 103.jpg 113.jpg 763.jpg 823.jpg 1027.jpg 857.jpg 837.jpg 923.jpg 464.jpg 860.jpg', '51.jpg 1322.jpg 1118.jpg 1335.jpg 887.jpg 1045.jpg 771.jpg 1287.jpg 554.jpg 703.jpg 354.jpg 814.jpg 334.jpg 46.jpg 699.jpg 274.jpg 492.jpg 614.jpg 526.jpg 330.jpg']\n",
      "Submission: tfidf_svc.csv\n"
     ]
    }
   ],
   "source": [
    "format_submission(preds, 'tfidf_svc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance 2000 * 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.64575131 2.         1.73205081 ... 2.23606798 2.         2.82842712]\n",
      " [2.44948974 1.73205081 2.         ... 2.         1.73205081 2.23606798]\n",
      " [2.64575131 2.         2.23606798 ... 2.23606798 1.41421356 2.82842712]\n",
      " [2.44948974 1.73205081 1.41421356 ... 2.         1.73205081 2.64575131]\n",
      " [2.44948974 1.73205081 1.41421356 ... 2.         1.73205081 2.64575131]]\n",
      "(2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "dist_svc = ed(predict_tags_trans, tags_test)\n",
    "print(dist_svc[:5])\n",
    "print(dist_svc.shape)\n",
    "np.save('dist_svc', dist_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
